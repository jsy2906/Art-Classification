{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc8985bb-fcfa-45a8-b363-101b4024d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b4380b-82ef-46f0-bb97-214a6da260aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 227\n",
    "height = 227\n",
    "channel = 3\n",
    "autotune = tf.data.experimental.AUTOTUNE\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "878e7eef-0180-4aaa-b59f-3b88150f2c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1698 files belonging to 7 classes.\n",
      "Using 1189 files for training.\n",
      "Found 1698 files belonging to 7 classes.\n",
      "Using 509 files for validation.\n"
     ]
    }
   ],
   "source": [
    "path = 'C:/Users/pc/Desktop/20171787/AI/art_classification'\n",
    "\n",
    "train = os.path.join(path, 'train')\n",
    "trainset = tf.keras.preprocessing.image_dataset_from_directory(train, \n",
    "                                                           image_size=(weight, height),\n",
    "                                                           validation_split=.3,\n",
    "                                                           subset='training',\n",
    "                                                           seed=seed, batch_size=10)\n",
    "trainset = trainset.cache().prefetch(autotune)\n",
    "\n",
    "valset = tf.keras.preprocessing.image_dataset_from_directory(train, \n",
    "                                                           image_size=(weight, height),\n",
    "                                                           validation_split=.3,\n",
    "                                                           subset='validation',\n",
    "                                                           seed=seed, batch_size=10)\n",
    "valset = valset.cache().prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a517f845-1745-49fc-9c55-2334ce903eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = os.path.join(path, 'test/0')\n",
    "test = os.listdir(test)\n",
    "test_df = pd.DataFrame(test, columns=['file name'])\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e089246-8ef9-49de-b5e3-f3c2c5168043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강\n",
    "augmentor = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(input_shape = (weight, height, channel)),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.3),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "    ])\n",
    "\n",
    "# 데이터 정규화\n",
    "norm = tf.keras.layers.experimental.preprocessing.Rescaling(1/255)\n",
    "\n",
    "# 모델 생성\n",
    "vgg16 = tf.keras.applications.VGG16(include_top=False, input_shape=(weight, height, channel))\n",
    "vgg16.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    augmentor,\n",
    "    norm,\n",
    "    vgg16,\n",
    "    tf.keras.layers.GlobalAvgPool2D(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience =30)\n",
    "\n",
    "model.compile(optimizer='adam', loss='SparseCategoricalCrossentropy', metrics=['acc'])\n",
    "model.fit(trainset, validation_data=valset, epochs=100, callbacks=es)\n",
    "\n",
    "# Accuracy와 Loss 그래프로 시각화\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Acc vs Val_Acc')\n",
    "plt.plot(epochs, acc, label = 'acc')\n",
    "plt.plot(epochs, val_acc, label = 'val_acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Loss vs Val_Loss')\n",
    "plt.plot(epochs, loss, label = 'loss')\n",
    "plt.plot(epochs, val_loss, label = 'val_loss')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
